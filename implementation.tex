
\section{Implementation}
In this section we show how we implemented each of the components in GTEcho key value store.
We build up on Echo~\cite{echo} codebase to realize our hybrid vector clocks based distributed transactions.
During the implementation process some of the planned work did not worked out as expected. 
We discuss such incomplete components here in detail as well.


\subsection{Local GTEcho}

We first describe the implementation details of local transactions in GTEcho. 
The challenge is to integrate hardware clock based timestamping to an OCC based
MVCC engine. We describe key data-structures of GTEcho and key modificaiton we 
carried out for achieving our goal.

\begin{figure}[]   
	\centering
	\includegraphics[width=\linewidth]{figures/echo_ds.png} 
	\caption{The main data-structure of echo is a hash-table. Each hash entry points to a list of versioned data entries. This diagram is from ~\cite{echo}} 
	\label{fig:echods} 
\end{figure}

GTEcho key value store consists of one main data structure -- a hash table that can maintain versioned data objects. 
There are two flavours of runtime hash table structures. First is the hash-table layout for the 
persistence data storage. The keys in the hash table points to a list of version-entry structures ~\ref{fig:echods}.
Updating/appending new entries to the list involves crash-consistent NVM update steps. We do not describe them as they
are out-of-scope in terms of our current problem focus.

Second, each worker thread maintain a volatile hash-table structure to keep track of their speculative read/write sets.
Instead of multiple version entries, the table maintain only one version -- the read/write set does not need multiple versions.
Furthermore, volatile hash-table does not follow NVM crash-consistent semantics during updates.

At the start of the speculative execution worker thread reads the \[ T_R \] and cache it in the local workspace. The subsequent
reads that are not already cached get fetched from the local NVM resident hash-table based on data visibility version checks.
During the commit time we lock each of the write-set entries and compare them with with our commit timestamp \[T_C\] for 
conflics. -- if no conflicts the worker merges the local chenges in to NVM resident master hash-table.

We ported the original Echo kvstore to use our hardware clock based local timestamping. The  port took around 100 lines
of code. The Echo code has roughly 8000 lines of C code and the authors do not describe much of the internals related to
concurrency in their paper.

\subsection{Distributed GTEcho}

During a distributed transaction involving two or more partitions of GTEcho, the corresponding worker thread has to 
interact with the remote GTEcho instances in-addition to local one. However, the basic steps of read fetch, write-buffering
and commit/merge does not change. Extending GTEcho to interact with remote paritions includes creating RDMA based RPC 
client/server communication abstraction. We use RPC over one-sided RDMA read/writes because of ease of application porting.
RPC based interface requires minimal changes to existing data structures.

Our first choice was to use Existing RDMA RPC~\cite{drtmh} library for the GTEcho port. We chose recent work named ROCC~\cite{rocc} 
as the library code is available open-source and claim ease of use. The library would have been an ideal match as it
implements RDMA RPC over UD -- a key peformnce enhacing technique. However, our attempts to get it working for even
simple RPC call failed. The found out that the core RDMA code base is has strong dependencies to the other code pieces
where they implement distributed transactions. 

Next, we tried building our own RDMA RPC library. We used RDMA RC send/receive primitives supported by ibverbs 
library to creeate a initial working RPC client/server. However, we failed to integrate the code piece in to GTEcho
due to time constraints. We managed to achieve throughput of x over a simple echo message use y bytes as RPC header
on our testbed machine.

\subsection{Failed Attempts}

Our first choice of implementing proposed timestamping technique was to use NAM-DB code base. They already
implement vector clock based timestamps and has distributed transactions support. However, we found out
that NAM-DB codebase was taken out of its open source hosting site due to interllectual propery issues.
We contacted the author of the work, but he denied sharing the code base because of the same.

\section{Evaluation}

We evaluate GTEcho on our in-house cluster ~\ref{tab:eval}. We only have node local GTEcho in the working
state due to implementation chanllenges described before. Hence the following experiments 
will show the effectiveness of hardware clock based timestamping -- a critical component 
in our proposed loosely synchronized hybrid vector clcock timestamps.

\begin{table}[]
	\begin{tabular}{l|l}
		\hline
		Compute cores	&  Intel Xeon X5650, 24 cores on two sockets. \\
		No. of nodes	  &  2  \\
		Main memory	  &  47GB DRAM \\
		NVM emulation & /dev/shm \\
		RDMA NIC		  & Mellanox Technologies MT26428  \\
		\hline
	\end{tabular}
	\caption{Evaluation testbed configuration consists of two physical nodes connected via RDMA fabric} 
	\label{tab:eval} 
\end{table}

We use Echo timestamping mechanism as our baseline to compare against. Echo uses global version counter as the timestamp. 
WHISPER~\cite{whisper} used Echo as an application benchmark and used a three workload classes named small, medium and large.
We use the same benchmark configuration in our runs and report the average of 3 runs in the experiments.

\begin{figure}[]   
	\centering
	\includegraphics[width=\linewidth]{figures/chart.png} 
	\caption{\small } 
	\label{fig:eval} 
\end{figure}





\section{Related Work}
In this section we list down the most recent and important work that we 
found related to our work. 

\noindent{Multi-core transactions}
With widespread use of multi-core machines, and terabytes of main memory
it is possible to run complete data management system within a single 
physical node. Cicada~\cite{cicada} implments an in-memory, serializable
transaction processing database with multi-version concurrency control. They
introduce vector clocks based time-stamping for multi-cores. Echo~\cite{echo}
implements MVCC based key-value store for NVM based multi-core systems. Echo 
uses a global timestamp for versioning. GTEcho differs from these works as
our main goal is to scale time-stamping both within single node and across nodes.

\noindent{Distributed transactions}
FARM~\cite{farm} implements a distributed shared memory programming using distributed 
transactions. There system is heavily optimized for one-sided RDMA operations.
Fasst~\cite{fasst} also implements distributed RDMA based trasactions, but with 
optimized RDMA based RPC. Specifically they use unreliable datagram connection
for fast RPC using RDMA. Both of the above uses conventional 2PL based concurrency
control for distributed transactions. NAM-DB~\cite{namdb} implements OCC based 
distribtued transactions using RDMA primtives. They scale cross node timestamping
using per-thread based vector clocks. They do not optimize timestamping for 
partitioned data stores ( compute is always remote to the data).

\noindent{Hardware timestamps}
Spanner~\cite{spanner} implements strictly serializable distributed transactions
using synchronized hardware clocks and introduces Truetime abstractions for 
timestamp based ordering. GTecho uses hybrid timestamps where cross node operation
ordering and optimize local node operation ordering using commodity hardware clocks.

To the best of our knowledge GTEcho is the only system that uses hybrid vector
clock based timestamps to accelerate both local and distribtued transactions.



\section{Discussion}
