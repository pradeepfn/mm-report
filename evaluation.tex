\section{evaluation}

\subsection{Experimental setup}
We emulate NVRAM characteristics using available DRAM in the experiment nodes.
The file backed \textit{ mmap()} system call allocate specific memory regions that
get treated as NVRAM during libary operations. We host the mmapped file in tempfs (in memory file system) 
in order to achieve dram speeds while persistent read/write to the file. The mmaped memory regions allow us 
to treat the emulated NVRAM as virtual memory while \textit{tempfs} hosted backed file
gurantees the data persistent over application restarts.  


\begin{figure}[ht]
\centering
\includegraphics[keepaspectratio,width=1\linewidth]{plot/evaluation.eps}
\caption{dummy caption}
\label{fig:evaluation1}
\end{figure}

Furthermore we emulate the bandwidth characteristics of NVRAM using delays during the memory copy operations.
The data write and write badnwidth utilization happens during
the checkpoint stage of the applications while data read and read bandwidth utilization happens during restart stage.
Since restart performance is the topic in focus, the data read latency and the read bandwidth
are the most important emulation characteristics in this work. 

During the data copying we from NVRAM to DRAM we introduce a read bandwidth delay. The read data size(Mb) divided by 
NVRAM read bandwidth constant(Mb/s) would give us the delay in seconds. We derive the NVRAM read bandwidth constant
by running stream benchmark over our emulation code. We consider NVRAM read bandwidth to be 2 GB/s. Since DRAM/NVRAM
read latencies are projected to be similar, we use DRAM read latencies without emulation for this evaluation.

During the checkpoint operation we copy the data over to mmapped memory segments (emulated NVRAM) from the DRAM.
The call the \textit{msync()} after checkpoint data copy, gurantees the persistent writes in to the backing file.


 
\begin{table}[h!]
\centering
\begin{tabular}{l l} 
\hline
 Number of nodes & 2\\
 Interconnect & Infiniband\\
 CPU & Intel Xeon 2.67 GHz \\  
 CPU cores per node  & 24 \\  
 Total Main memory & 49 GB\\
 Emulated NVRAM & 20GB\\
 Effective Main memory & 29GB\\
 SSD & Intel, 120GB SATA\\
 MPI lib & MPICH2\\
 Operating system & RHEL 6.5 \\  
 compilers & gfortran, gcc\\
 \hline
\end{tabular}
\caption{Experimental setup}
\label{fig:evaluation1}
\end{table}



\begin{table}[h!]
\centering
\begin{tabular}{l l} 
\hline
 I/O strategy & memcopy/mmap\\
 Block size & 4K\\
 1 processes, 4GB data read & 253 MB/S\\
 8 processes, 512MB data read & 34 MB/S\\ 
 \hline
\end{tabular}
\caption{SSD bandwidth measurements using fio benchmark}
\label{fig:evaluation1}
\end{table}



\textbf{Applications:} We evaluate our C/R implementation against three real world HPC applications and observe the restart performance. The applications consists of algorithms related to different scientific fields and hence each application show unique characteristcs during C/R.  The work size is selected 
based on our computing resource capabilities.
\begin{enumerate}
\item gyrokinetic toroidal code (GTC) is a three dimensional particle-in-cell application used in, microturbulance fushion device studies. The checkpoint data constitutes of 2D/3D arrays. We vary the \textit{mpsi} value to obtain different checkpoint sizes. 
\item CM1 is a three dimensional non-hydrostatic atmospheric model for used in studies of atmospheric phenomena. CM1 application saves large number of variables(~50) compared to other applications during its checkpoints.
\item A direct numerical simulation (DNS) solver, S3D \cite{chen2009terascale} simulate the microscales of turbulent combustion. It solves full compressible Navierâ€“Stokes equations.


\end{enumerate}
